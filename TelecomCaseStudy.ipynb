{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896ade6d",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0322360",
   "metadata": {},
   "source": [
    "### Business objective : To predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a597a31",
   "metadata": {},
   "source": [
    "###### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a26bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from scipy import special \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "import math\n",
    "from IPython.display import Markdown, display ,HTML\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None) # make sure data and columns are displayed correctly withput purge\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb0dcc",
   "metadata": {},
   "source": [
    "###### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f94fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'telecom_churn_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10868/1230502609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtcom_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"telecom_churn_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'telecom_churn_data.csv'"
     ]
    }
   ],
   "source": [
    "tcom_data = pd.read_csv(\"telecom_churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 10 field with all columns in the dataset\n",
    "tcom_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a985f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429b63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1de4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a563d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244625b3",
   "metadata": {},
   "source": [
    "###### Checking the overall missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "((tcom_data.isnull().sum()/tcom_data.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all the columns with datetime format\n",
    "date_col= tcom_data.select_dtypes(include=['object'])\n",
    "print(\"\\nThese are the columns available with datetime format represented as object\\n\",date_col.columns)\n",
    "\n",
    "# Converting the selected columns to datetime format\n",
    "for i in date_col.columns:\n",
    "    tcom_data[i] = pd.to_datetime(tcom_data[i])\n",
    "\n",
    "# Current dimension of the dataset\n",
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3791a8",
   "metadata": {},
   "source": [
    "###### Handling missing values with respect to data recharge attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tcom_data[['date_of_last_rech_data_6','total_rech_data_6','max_rech_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e6135",
   "metadata": {},
   "source": [
    "##### date_of_last_rech_data, total_rech_data, max_rech_data has missing values. These columns represents the the customer has not done any recharge for mobile interenet. Imputing 0 as their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b710e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae20ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tcom_data)):\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 6\n",
    "    if pd.isnull((tcom_data['total_rech_data_6'][i]) and (tcom_data['max_rech_data_6'][i])):\n",
    "        if pd.isnull(tcom_data['date_of_last_rech_data_6'][i]):\n",
    "            tcom_data['total_rech_data_6'][i]=0\n",
    "            tcom_data['max_rech_data_6'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 7\n",
    "    if pd.isnull((tcom_data['total_rech_data_7'][i]) and (tcom_data['max_rech_data_7'][i])):\n",
    "        if pd.isnull(tcom_data['date_of_last_rech_data_7'][i]):\n",
    "            tcom_data['total_rech_data_7'][i]=0\n",
    "            tcom_data['max_rech_data_7'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 8\n",
    "    if pd.isnull((tcom_data['total_rech_data_8'][i]) and (tcom_data['max_rech_data_8'][i])):\n",
    "        if pd.isnull(tcom_data['date_of_last_rech_data_8'][i]):\n",
    "            tcom_data['total_rech_data_8'][i]=0\n",
    "            tcom_data['max_rech_data_8'][i]=0\n",
    "\n",
    "  # Handling 'total_rech_data', 'max_rech_data' and for month 9\n",
    "    if pd.isnull((tcom_data['total_rech_data_9'][i]) and (tcom_data['max_rech_data_9'][i])):\n",
    "        if pd.isnull(tcom_data['date_of_last_rech_data_9'][i]):\n",
    "            tcom_data['total_rech_data_9'][i]=0\n",
    "            tcom_data['max_rech_data_9'][i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55c399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf379aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data[['count_rech_2g_6','count_rech_3g_6','total_rech_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba219e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab278052",
   "metadata": {},
   "source": [
    "##### From the above tabular the column values of total_rech_data for each month from 6 to 9 respectively is the sum of the columns values of count_rech_2g for each month from 6 to 9 respectively and count_rech_3g for each month from 6 to 9 respectively, which derives to a multicollinearity issue. In order to reduce the multicollinearity, we can drop the columns count_rech_2g for each month from 6 to 9 respectively and count_rech_3g for each month from 6 to 9 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns 'count_rech_2g_*' & 'count_rech_3g_*' for the months 6,7,8 and 9 \n",
    "tcom_data.drop(['count_rech_2g_6','count_rech_3g_6',\n",
    "                   'count_rech_2g_7','count_rech_3g_7',\n",
    "                   'count_rech_2g_8','count_rech_3g_8',\n",
    "                   'count_rech_2g_9','count_rech_3g_9'],axis=1, inplace=True)\n",
    "\n",
    "print(\"The 'count_rech_2g_6','count_rech_3g_6','count_rech_2g_7','count_rech_3g_7','count_rech_2g_8','count_rech_3g_8','count_rech_2g_9','count_rech_3g_9' columns are dropped as they can be explained from the 'total_rech_data'column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9769c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fba94e",
   "metadata": {},
   "source": [
    "##### Columns with unique value 1 can be dropped as there will not be variance in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc609cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the columns unique values and drop such columns with its value as 1\n",
    "unique_1_col=[]\n",
    "for i in tcom_data.columns:\n",
    "    if tcom_data[i].nunique() == 1:\n",
    "        unique_1_col.append(i)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.drop(unique_1_col, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef57daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3ac57",
   "metadata": {},
   "source": [
    "##### Handling the missing values for the attributes arpu_3g_*,arpu_2g_* for month 6,7,8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the related columns values\n",
    "tcom_data[['arpu_3g_6','arpu_2g_6','av_rech_amt_data_6']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a343e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation table for month 6\\n\\n\", tcom_data[['arpu_3g_6','arpu_2g_6','av_rech_amt_data_6']].corr())\n",
    "print(\"\\nCorrelation table for month 7\\n\\n\", tcom_data[['arpu_3g_7','arpu_2g_7','av_rech_amt_data_7']].corr())\n",
    "print(\"\\nCorrelation table for month 8\\n\\n\", tcom_data[['arpu_3g_8','arpu_2g_8','av_rech_amt_data_8']].corr())\n",
    "print(\"\\nCorrelation table for month 9\\n\\n\", tcom_data[['arpu_3g_9','arpu_2g_9','av_rech_amt_data_9']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns 'arpu_3g_*'&'arpu_2g_*' in month 6,7,8 and 9 datafrom the dataset\n",
    "tcom_data.drop(['arpu_3g_6','arpu_2g_6',\n",
    "                  'arpu_3g_7','arpu_2g_7',\n",
    "                  'arpu_3g_8','arpu_2g_8',\n",
    "                  'arpu_3g_9','arpu_2g_9'],axis=1, inplace=True)\n",
    "print(\"\\nThe columns'arpu_3g_6','arpu_2g_6','arpu_3g_7','arpu_2g_7','arpu_3g_8','arpu_2g_8','arpu_3g_9','arpu_2g_9' are dropped from the dataset due to high corellation between their respective arpu_* variable in the dataset\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b92224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curent dimensions of the dataset\n",
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a0ceb",
   "metadata": {},
   "source": [
    "##### fb_user_* and night_pck_user_* for each month from 6 to 9 respectively has a missing values above 50%. So dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.drop(['fb_user_6','fb_user_7','fb_user_8','fb_user_9',\n",
    "                  'night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9'],\n",
    "                  axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'fb_user_6','fb_user_7','fb_user_8','fb_user_9','night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9' are dropped from the dataset as it has no meaning to the data snd has high missing values above 50%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd088155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the related columns values\n",
    "tcom_data[['av_rech_amt_data_7','max_rech_data_7','total_rech_data_7']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d82e9",
   "metadata": {},
   "source": [
    "##### missing values for the column av_rech_amt_data_* for each month from 6 to 9 can be replaced as 0 if the total_rech_data_* for each month from 6 to 9 respectively is 0. i.e. if the total recharge done is 0 then the average recharge amount shall also be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b611f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tcom_data)):\n",
    "  # Handling `av_rech_amt_data`  for month 6\n",
    "    if (pd.isnull(tcom_data['av_rech_amt_data_6'][i]) and (tcom_data['total_rech_data_6'][i]==0)):\n",
    "        tcom_data['av_rech_amt_data_6'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 7\n",
    "    if (pd.isnull(tcom_data['av_rech_amt_data_7'][i]) and (tcom_data['total_rech_data_7'][i]==0)):\n",
    "        tcom_data['av_rech_amt_data_7'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 8\n",
    "    if (pd.isnull(tcom_data['av_rech_amt_data_8'][i]) and (tcom_data['total_rech_data_8'][i]==0)):\n",
    "        tcom_data['av_rech_amt_data_8'][i] = 0\n",
    "\n",
    "  # Handling `av_rech_amt_data`  for month 9\n",
    "    if (pd.isnull(tcom_data['av_rech_amt_data_9'][i]) and (tcom_data['total_rech_data_9'][i]==0)):\n",
    "        tcom_data['av_rech_amt_data_9'][i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445b72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe864e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkng the overall missing values in the dataset\n",
    "((tcom_data.isnull().sum()/tcom_data.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a1026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccadac",
   "metadata": {},
   "source": [
    "##### date_of_last_rech_data_* corresponding to months 6,7,8 and 9 are of no value after the conditional imputation of of columns total_rech_data_*, max_rech_data_*are completes. missing value percentage is high for these columns and can be dropped from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0935ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns related to datetime dtype from the dataset\n",
    "tcom_data.drop([\"date_of_last_rech_data_6\",\"date_of_last_rech_data_7\",\n",
    "                   \"date_of_last_rech_data_8\",\"date_of_last_rech_data_9\"], axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_data_9' are dropped as it has no significance to the data\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2513c6",
   "metadata": {},
   "source": [
    "##### we can drop the date_of_last_rech_data_* column corresponding to months 6,7,8 and 9 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bded8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns related to datetime dtype from the dataset\n",
    "tcom_data.drop([\"date_of_last_rech_6\",\"date_of_last_rech_7\",\n",
    "                   \"date_of_last_rech_8\",\"date_of_last_rech_9\"], axis=1, inplace=True)\n",
    "print(\"\\nThe columns 'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9' are dropped as it has no significance to the data\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curent dimensions of the dataset\n",
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab98aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae037cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d70c1b0",
   "metadata": {},
   "source": [
    "### 1. Filter high-value customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data \n",
    " # We are filtering the data in accordance to total revenue generated per customer.\n",
    "\n",
    " # first we need the total amount recharge amount done for data alone, we have average rechage amount done. \n",
    "\n",
    " # Calculating the total recharge amount done for data alone in months 6,7,8 and 9\n",
    "tcom_data['total_rech_amt_data_6']=tcom_data['av_rech_amt_data_6'] * tcom_data['total_rech_data_6']\n",
    "tcom_data['total_rech_amt_data_7']=tcom_data['av_rech_amt_data_7'] * tcom_data['total_rech_data_7']\n",
    "\n",
    "# Calculating the overall recharge amount for the months 6,7,8 and 9\n",
    "tcom_data['overall_rech_amt_6'] = tcom_data['total_rech_amt_data_6'] + tcom_data['total_rech_amt_6']\n",
    "tcom_data['overall_rech_amt_7'] = tcom_data['total_rech_amt_data_7'] + tcom_data['total_rech_amt_7']\n",
    "\n",
    "# Calculating the average recharge done by customer in months June and July(i.e. 6th and 7th month)\n",
    "tcom_data['avg_rech_amt_6_7'] = (tcom_data['overall_rech_amt_6'] + tcom_data['overall_rech_amt_7'])/2\n",
    "\n",
    "# Finding the value of 70th percentage in the overall revenues defining the high value customer creteria for the company\n",
    "cut_off = tcom_data['avg_rech_amt_6_7'].quantile(0.70)\n",
    "print(\"\\nThe 70th quantile value to determine the High Value Customer is: \",cut_off,\"\\n\")\n",
    "\n",
    "# Filtering the data to the top 30% considered as High Value Customer\n",
    "tcom_data = tcom_data[tcom_data['avg_rech_amt_6_7'] >= cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70890abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the missing values percentages again for the HVC group\n",
    "    # Checkng the overall missing values in the dataset\n",
    "((tcom_data.isnull().sum()/tcom_data.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns available\n",
    "num_col = tcom_data.select_dtypes(include = ['int64','float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e816126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for Scaling and Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calling the Scaling function\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "# Scaling and transforming the data for the columns that are numerical\n",
    "tcom_data[num_col]=scalar.fit_transform(tcom_data[num_col])\n",
    "\n",
    "# Calling the KNN Imputer function\n",
    "knn=KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Imputing the NaN values using KNN Imputer\n",
    "\n",
    "tcom_data_knn = pd.DataFrame(knn.fit_transform(tcom_data[num_col]))\n",
    "tcom_data_knn.columns=tcom_data[num_col].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c67134",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data_knn.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f641966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the scaled data back to the original data\n",
    "tcom_data[num_col]=scalar.inverse_transform(tcom_data_knn)\n",
    "\n",
    "# Checking the top 10 data\n",
    "tcom_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall missing values in the dataset\n",
    "((tcom_data.isnull().sum()/tcom_data.shape[0])*100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0e982",
   "metadata": {},
   "source": [
    "### 2. Tag churners and remove attributes of the churn phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c92e1",
   "metadata": {},
   "source": [
    "##### derive churn variable using total_ic_mou_9,total_og_mou_9,vol_2g_mb_9 and vol_3g_mb_9 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns to define churn variable (i.e. TARGET Variable)\n",
    "churn_col=['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']\n",
    "tcom_data[churn_col].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71464a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the churn variable.\n",
    "tcom_data['churn']=0\n",
    "\n",
    "# Imputing the churn values based on the condition\n",
    "tcom_data['churn'] = np.where(tcom_data[churn_col].sum(axis=1) == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c00843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a697358",
   "metadata": {},
   "source": [
    "###### Performing EDA for Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac58621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#churn/non churn percentage\n",
    "print((tcom_data['churn'].value_counts()/len(tcom_data))*100)\n",
    "((tcom_data['churn'].value_counts()/len(tcom_data))*100).plot(kind=\"pie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c4d7f",
   "metadata": {},
   "source": [
    "###### Recharge amount related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_amount_columns =  tcom_data.columns[tcom_data.columns.str.contains('rech_amt')]\n",
    "recharge_amount_columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eebd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_chart(attribute):\n",
    "    plt.figure(figsize=(20,16))\n",
    "    df = tcom_data\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.boxplot(data=df, y=attribute+\"_6\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.boxplot(data=df, y=attribute+\"_7\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.boxplot(data=df, y=attribute+\"_8\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('max_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc15f66",
   "metadata": {},
   "source": [
    "###### There is a drop in the max recharge amount for churned customers in the 8th Month (Action Phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=tcom_data, y='avg_rech_amt_6_7',x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d558126",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=tcom_data, y='av_rech_amt_data_8',x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=tcom_data, y='overall_rech_amt_6',x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b937ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=tcom_data, y='overall_rech_amt_7',x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b747586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('vol_2g_mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('vol_3g_mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad73c57",
   "metadata": {},
   "source": [
    "###### 2G and 3G usage for churned customers drops in 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b90d2",
   "metadata": {},
   "source": [
    "###### It can be observed that 2G/3G usage is higher for non-churned customers indicating that churned customers might be from areas where 2G/3G service is not properly available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_bar_chart(df,columns_list):\n",
    "    df_0 = df[df.churn==0].filter(columns_list)\n",
    "    df_1 = df[df.churn==1].filter(columns_list)\n",
    "\n",
    "    mean_df_0 = pd.DataFrame([df_0.mean()],index={'Non Churn'})\n",
    "    mean_df_1 = pd.DataFrame([df_1.mean()],index={'Churn'})\n",
    "\n",
    "    frames = [mean_df_0, mean_df_1]\n",
    "    mean_bar = pd.concat(frames)\n",
    "\n",
    "    mean_bar.T.plot.bar(figsize=(10,5),rot=0)\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "vbc_column = tcom_data.columns[tcom_data.columns.str.contains('vbc_3g',regex=True)]\n",
    "vbc_column.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar_chart(tcom_data, vbc_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792673b",
   "metadata": {},
   "source": [
    "###### It can be observed that volume-based cost for 3G is much lower for Churned customers as compared to Non-Churned Customers and there is a drop in vbc in 8th month also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55faf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_2g_or_3g_col = tcom_data.columns[tcom_data.columns.str.contains('sachet_2g|sachet_3g',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar_chart(tcom_data, SC_2g_or_3g_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765236ec",
   "metadata": {},
   "source": [
    "###### drop in sachet services in 8th month for churned cutsomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns for average revenue per user\n",
    "arpu_cols = tcom_data.columns[tcom_data.columns.str.contains('arpu_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2998719",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpu_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d013f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('arpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca9895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar_chart(tcom_data, arpu_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea09a1",
   "metadata": {},
   "source": [
    "###### there is drop for Arpu in 8th month for churned customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "offnet_usage_service_col = tcom_data.columns[tcom_data.columns.str.contains('offnet.*mou',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6220eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "offnet_usage_service_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('offnet_mou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar_chart(tcom_data, offnet_usage_service_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a28db",
   "metadata": {},
   "source": [
    "###### There is a drop for offnet mou services in the 8th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76175e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnet_usage_service =  tcom_data.columns[tcom_data.columns.str.contains('^onnet.*mou',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2370373",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnet_usage_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('onnet_mou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar_chart(tcom_data, onnet_usage_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bdfc1",
   "metadata": {},
   "source": [
    "###### there is a drop in onnet_usage_Service in the 8th month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1b43e",
   "metadata": {},
   "source": [
    "#### Proceeding with further data preparation, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2677b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_phase_cols = [col for col in tcom_data.columns if '_9' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the selected churn phase columns\n",
    "tcom_data.drop(churn_phase_cols, axis=1, inplace=True)\n",
    "\n",
    "# The curent dimension of the dataset after dropping the churn related columns\n",
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.drop(['total_rech_amt_data_6','av_rech_amt_data_6',\n",
    "                   'total_rech_data_6','total_rech_amt_6',\n",
    "                  'total_rech_amt_data_7','av_rech_amt_data_7',\n",
    "                   'total_rech_data_7','total_rech_amt_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of column names for each month\n",
    "mon_6_cols = [col for col in tcom_data.columns if '_6' in col]\n",
    "mon_7_cols = [col for col in tcom_data.columns if '_7' in col]\n",
    "mon_8_cols = [col for col in tcom_data.columns if '_8' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29731848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the correlation amongst the independent variables, drop the highly correlated ones\n",
    "tcom_data_corr = tcom_data.corr()\n",
    "tcom_data_corr.loc[:,:] = np.tril(tcom_data_corr, k=-1)\n",
    "tcom_data_corr = tcom_data_corr.stack()\n",
    "tcom_data_corr\n",
    "tcom_data_corr[(tcom_data_corr > 0.80) | (tcom_data_corr < -0.80)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7823e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop=['total_rech_amt_8','isd_og_mou_8','isd_og_mou_7','sachet_2g_8','total_ic_mou_6',\n",
    "            'total_ic_mou_8','total_ic_mou_7','std_og_t2t_mou_6','std_og_t2t_mou_8','std_og_t2t_mou_7',\n",
    "            'std_og_t2m_mou_7','std_og_t2m_mou_8',]\n",
    "\n",
    "# These columns can be dropped as they are highly collinered with other predictor variables.\n",
    "# criteria set is for collinearity of 85%\n",
    "\n",
    "#  dropping these column\n",
    "tcom_data.drop(col_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new variable 'tenure'\n",
    "tcom_data['tenure'] = (tcom_data['aon']/30).round(0)\n",
    "\n",
    "# Since we derived a new column from 'aon', we can drop it\n",
    "tcom_data.drop('aon',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_range = [0, 6, 12, 24, 60, 61]\n",
    "tn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "tcom_data['tenure_range'] = pd.cut(tcom_data['tenure'], tn_range, labels=tn_label)\n",
    "tcom_data['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar plot for tenure range\n",
    "plt.figure(figsize=[12,7])\n",
    "sns.barplot(x='tenure_range',y='churn', data=tcom_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeccc4c",
   "metadata": {},
   "source": [
    "##### It can be observed that Most Churn happens during the first 6 months. As a customer stays longer with the network, Churn decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c640b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495160d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tcom_data['tenure'],bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c7552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ed47b5",
   "metadata": {},
   "source": [
    "##### The average revenue per user in good phase of customer is given by arpu_6 and arpu_7. since we have two seperate averages, lets take an average to these two and drop the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data[\"avg_arpu_6_7\"]= (tcom_data['arpu_6']+tcom_data['arpu_7'])/2\n",
    "tcom_data['avg_arpu_6_7'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56196ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.drop(['arpu_6','arpu_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b130bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tcom_data['avg_arpu_6_7'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb41e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,50))\n",
    "heatmap_churn = sns.heatmap(tcom_data.corr()[['churn']].sort_values(ascending=False, by='churn'),annot=True, \n",
    "                                cmap='summer')\n",
    "heatmap_churn.set_title(\"Features Correlating with Churn variable\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5bbbc",
   "metadata": {},
   "source": [
    "##### Avg Outgoing Calls & calls on romaning for 6 & 7th months are positively correlated with churn. Avg Revenue, No. Of Recharge for 8th month has negative correlation with churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7aa1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = tcom_data.churn, y = tcom_data.tenure)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcom_data[['total_rech_num_8', 'arpu_8']].plot.scatter(x = 'total_rech_num_8',\n",
    "                                                              y='arpu_8')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot between churn vs max rechare amount\n",
    "ax = sns.kdeplot(tcom_data.max_rech_amt_8[(tcom_data[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(tcom_data.max_rech_amt_8[(tcom_data[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Max Recharge Amount by churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn vs max rechare amount\n",
    "ax = sns.kdeplot(tcom_data.av_rech_amt_data_8[(tcom_data[\"churn\"] == 0)],\n",
    "                color=\"Red\", shade = True)\n",
    "ax = sns.kdeplot(tcom_data.av_rech_amt_data_8[(tcom_data[\"churn\"] == 1)],\n",
    "                ax =ax, color=\"Blue\", shade= True)\n",
    "ax.legend([\"No-Churn\",\"Churn\"],loc='upper right')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlabel('Volume based cost')\n",
    "ax.set_title('Distribution of Average Recharge Amount for Data by churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating categories for month 8 column totalrecharge and their count\n",
    "tcom_data['total_rech_data_group_8']=pd.cut(tcom_data['total_rech_data_8'],[-1,0,10,25,100],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])\n",
    "tcom_data['total_rech_num_group_8']=pd.cut(tcom_data['total_rech_num_8'],[-1,0,10,25,1000],labels=[\"No_Recharge\",\"<=10_Recharges\",\"10-25_Recharges\",\">25_Recharges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a386861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=tcom_data,x=\"total_rech_data_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_data_8 variable\\n\",tcom_data['total_rech_data_group_8'].value_counts())\n",
    "plt.show()\n",
    "plt.figure(figsize=[12,4])\n",
    "sns.countplot(data=tcom_data,x=\"total_rech_num_group_8\",hue=\"churn\")\n",
    "print(\"\\t\\t\\t\\t\\tDistribution of total_rech_num_8 variable\\n\",tcom_data['total_rech_num_group_8'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65d017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy variable for some of the categorical variables and dropping the first one.\n",
    "dummy = pd.get_dummies(tcom_data[['total_rech_data_group_8','total_rech_num_group_8','tenure_range']], drop_first=True)\n",
    "dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the results to the master dataframe\n",
    "tcom_data = pd.concat([tcom_data, dummy], axis=1)\n",
    "tcom_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78781b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the filtered dataframe\n",
    "\n",
    "df=tcom_data[:].copy()\n",
    "\n",
    "# Dropping unwanted columns\n",
    "df.drop(['tenure_range','mobile_number','total_rech_data_group_8','total_rech_num_group_8','sep_vbc_3g','tenure'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425850f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create X dataset for model building.\n",
    "X = df.drop(['churn'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0913ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622869b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['churn']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afda4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dateset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n",
    "print(\"Dimension of X_train:\", X_train.shape)\n",
    "print(\"Dimension of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ac2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = X_train.select_dtypes(include = ['int64','float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c176242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling on the dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[num_col] = scaler.fit_transform(X_train[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6800245",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e46d7",
   "metadata": {},
   "source": [
    "#### since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - using SMOTE technique to handle class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d918db",
   "metadata": {},
   "source": [
    "##### Handling Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of X_train_sm Shape:\", X_train_sm.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train_sm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf32595",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d83862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for Model creation\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train_sm,(sm.add_constant(X_train_sm)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eecb705",
   "metadata": {},
   "source": [
    "##### Using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc71c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# running RFE with 20 variables as output\n",
    "rfe = RFE(logreg, step = 20)             \n",
    "rfe = rfe.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d42551",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns=X_train_sm.columns[rfe.support_]\n",
    "print(\"The selected columns by RFE for modelling are: \\n\\n\",rfe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23324c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_sm.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the p-value of the individual columns, \n",
    "    # we can drop the column 'loc_ic_t2t_mou_8' as it has high p-value of 0.80\n",
    "rfe_columns_1=rfe_columns.drop('loc_ic_t2t_mou_8',1)\n",
    "print(\"\\nThe new set of edited featured are:\\n\",rfe_columns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the edited feature list\n",
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns_1])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the p-value of the individual columns, \n",
    "    # we can drop the column 'loc_ic_t2m_mou_8' as it has high p-value of 0.80\n",
    "rfe_columns_2=rfe_columns_1.drop('loc_ic_t2m_mou_8',1)\n",
    "print(\"\\nThe new set of edited featured are:\\n\",rfe_columns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the edited feature list\n",
    "X_train_SM = sm.add_constant(X_train_sm[rfe_columns_2])\n",
    "logm2 = sm.GLM(y_train_sm,X_train_SM, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bae8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_sm_pred = res.predict(X_train_SM)\n",
    "y_train_sm_pred = y_train_sm_pred.values.reshape(-1)\n",
    "y_train_sm_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdcbc1",
   "metadata": {},
   "source": [
    "##### Creating a dataframe with the actual churn flag and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final = pd.DataFrame({'Converted':y_train_sm.values, 'Converted_prob':y_train_sm_pred})\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6719ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating new column 'churn_pred' with 1 if Churn_Prob > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90988d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final['churn_pred'] = y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Viewing the prediction results\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3a03e",
   "metadata": {},
   "source": [
    "##### Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy.\n",
    "print(\"The overall accuracy of the model is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.churn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables. \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_sm[rfe_columns_2].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_sm[rfe_columns].values, i) for i in range(X_train_sm[rfe_columns_2].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP / float(TP+FN))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN / float(TN+FP))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP/ float(TN+FP))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP / float(TP+FP))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee750332",
   "metadata": {},
   "source": [
    "##### Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38032b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to plot the roc curve\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Prediction Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b110fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdaa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with refined probability cutoffs \n",
    "numbers = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in numbers:\n",
    "    y_train_sm_pred_final[i]= y_train_sm_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaad906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.50,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy sensitivity and specificity for various probabilities calculated above.\n",
    "cutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257345c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_sm_pred_final['final_churn_pred'] = y_train_sm_pred_final.Converted_prob.map( lambda x: 1 if x > 0.52 else 0)\n",
    "\n",
    "y_train_sm_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3752a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the ovearall accuracy again\n",
    "print(\"The overall accuracy of the model now is:\",metrics.accuracy_score(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29516bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_sm_pred_final.Converted, y_train_sm_pred_final.final_churn_pred )\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP2 = confusion2[1,1] # true positive \n",
    "TN2 = confusion2[0,0] # true negatives\n",
    "FP2 = confusion2[0,1] # false positives\n",
    "FN2 = confusion2[1,0] # false negatives\n",
    "\n",
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP2 / float(TP2+FN2))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN2 / float(TN2+FP2))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP2/ float(TN2+FP2))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP2 / float(TP2+FP2))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN2 / float(TN2 + FN2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = precision_recall_curve(y_train_sm_pred_final.Converted, y_train_sm_pred_final.Converted_prob)\n",
    "\n",
    "# Plotting the curve\n",
    "plt.plot(thresholds, p[:-1], \"g-\")\n",
    "plt.plot(thresholds, r[:-1], \"r-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fffd2",
   "metadata": {},
   "source": [
    "##### Making Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test data\n",
    "X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a355d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "X_test=X_test[rfe_columns_2]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to the test model.\n",
    "X_test_SM = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96126d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(X_test_SM)\n",
    "print(\"\\n The first ten probability value of the prediction are:\\n\",y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f027d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_test_pred)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred.rename(columns = {0:\"Conv_prob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad39382",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = pd.concat([y_test_df,y_pred],axis=1)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['test_churn_pred'] = y_pred_final.Conv_prob.map(lambda x: 1 if x>0.54 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy of the predicted set.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a539ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "confusion2_test = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.test_churn_pred)\n",
    "print(\"Confusion Matrix\\n\",confusion2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating model validation parameters\n",
    "TP3 = confusion2_test[1,1] # true positive \n",
    "TN3 = confusion2_test[0,0] # true negatives\n",
    "FP3 = confusion2_test[0,1] # false positives\n",
    "FN3 = confusion2_test[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "print(\"Sensitivity = \",TP3 / float(TP3+FN3))\n",
    "\n",
    "# Let us calculate specificity\n",
    "print(\"Specificity = \",TN3 / float(TN3+FP3))\n",
    "\n",
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(\"False Positive Rate = \",FP3/ float(TN3+FP3))\n",
    "\n",
    "# positive predictive value \n",
    "print (\"Precision = \",TP3 / float(TP3+FP3))\n",
    "\n",
    "# Negative predictive value\n",
    "print (\"True Negative Prediction Rate = \",TN3 / float(TN3+FN3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08516c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of the predicted model is: \",round(metrics.accuracy_score(y_pred_final.churn, y_pred_final.test_churn_pred),2)*100,\"%\")\n",
    "print(\"The sensitivity of the predicted model is: \",round(TP3 / float(TP3+FN3),2)*100,\"%\")\n",
    "\n",
    "print(\"\\nAs the model created is based on a sensitivity model, i.e. the True positive rate is given more importance as the actual and prediction of churn by a customer\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for the test dataset\n",
    "\n",
    "# Defining the variables to plot the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.churn,y_pred_final.Conv_prob, drop_intermediate = False )\n",
    "# Plotting the curve for the obtained metrics\n",
    "draw_roc(y_pred_final.churn,y_pred_final.Conv_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0e8ef",
   "metadata": {},
   "source": [
    "### Logistic Regression using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56381360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dateset into train and test datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)\n",
    "print(\"Dimension of X_train:\", X_train.shape)\n",
    "print(\"Dimension of X_test:\", X_test.shape)\n",
    "\n",
    "# apply scaling on the dataset\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train[num_col] = scaler.fit_transform(X_train[num_col])\n",
    "X_test[num_col] = scaler.transform(X_test[num_col])\n",
    "\n",
    "# Applying SMOTE technique for data imbalance correction\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm,y_train_sm = sm.fit_resample(X_train,y_train)\n",
    "print(\"Dimension of X_train_sm Shape:\", X_train_sm.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train_sm.shape)\n",
    "\n",
    "X_train_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ef2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=42)\n",
    "\n",
    "# applying PCA on train data\n",
    "pca.fit(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm_pca=pca.fit_transform(X_train_sm)\n",
    "print(\"Dimension of X_train_sm_pca: \",X_train_sm_pca.shape)\n",
    "\n",
    "X_test_pca=pca.transform(X_test)\n",
    "print(\"Dimension of X_test_pca: \",X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the PCA components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e06292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649764b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg_pca = LogisticRegression()\n",
    "logreg_pca.fit(X_train_sm_pca, y_train_sm)\n",
    "\n",
    "# making the predictions\n",
    "y_pred = logreg_pca.predict(X_test_pca)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "print(\"Dimension of y_pred_df:\", y_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1acbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Checking the Confusion matrix\n",
    "print(\"Confusion Matirx for y_test & y_pred\\n\",confusion_matrix(y_test,y_pred),\"\\n\")\n",
    "\n",
    "# Checking the Accuracy of the Predicted model.\n",
    "print(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,len(pca.explained_variance_ratio_)+1),pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Making a scree plot\n",
    "fig = plt.figure(figsize=[12,7])\n",
    "plt.plot(var_cumu)\n",
    "plt.xlabel('no of principal components')\n",
    "plt.ylabel('explained variance - cumulative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642a6ee",
   "metadata": {},
   "source": [
    "##### **90% of the data can be explained with 8 PCA components*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b92de",
   "metadata": {},
   "source": [
    "##### **Fitting the dataset with the 8 explainable components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_8 = PCA(n_components=15)\n",
    "\n",
    "train_pca_8 = pca_8.fit_transform(X_train_sm)\n",
    "print(\"Dimension for Train dataset using PCA: \", train_pca_8.shape)\n",
    "\n",
    "test_pca_8 = pca_8.transform(X_test)\n",
    "print(\"Dimension for Test dataset using PCA: \", test_pca_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pca_8 = LogisticRegression()\n",
    "logreg_pca_8.fit(train_pca_8, y_train_sm)\n",
    "\n",
    "# making the predictions\n",
    "y_pred_8 = logreg_pca_8.predict(test_pca_8)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_df_8 = pd.DataFrame(y_pred_8)\n",
    "print(\"Dimension of y_pred_df_8: \", y_pred_df_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c71e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "print(\"Confusion Matirx for y_test & y_pred\\n\",confusion_matrix(y_test,y_pred_8),\"\\n\")\n",
    "\n",
    "# Checking the Accuracy of the Predicted model.\n",
    "print(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fee145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32f9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5253da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b19e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
